# -*- coding: utf-8 -*-
"""NLTK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P1XxqePdiBJzr5AAb09rvauiZRRJzGQb
"""

import nltk
nltk.download('all')

text = """The story begins in the Satya Yuga. King Daksha Prajapati, the son of Lord Brahma, was performing a grand Yagna (sacrificial ritual). He invited all the gods, sages, and kings, but intentionally excluded his own daughter, Sati, and her husband, Lord Shiva. Daksha despised Shiva’s ascetic lifestyle—his wandering in cremation grounds and his necklaces of snakes.

Despite Shiva’s warnings, Sati arrived at her father’s palace, hoping it was an oversight. Instead, she was met with public humiliation and insults directed at her husband. Unable to bear the dishonor, Sati invoked her yogic powers and immolated herself in the sacrificial fire.

When Shiva heard of Sati’s death, his grief turned into a world-ending rage. He performed the Tandava with Sati’s body on his shoulder. To save the universe from destruction, Lord Vishnu used his Sudarshana Chakra to sever Sati’s body into pieces, which fell across the Indian subcontinent."""

"""**1. Tokenization**"""

from nltk import word_tokenize ,sent_tokenize

nltk.sent_tokenize(text)

words=nltk.word_tokenize(text)
words

"""**Stemming **"""

from nltk.stem import PorterStemmer
PS = PorterStemmer()

for i in words:
  print(PS.stem(i))

"""**Lemmatization **"""

from nltk.stem import WordNetLemmatizer
lz=WordNetLemmatizer()

for i in words:
  print(lz.lemmatize(i,'v'))

"""**Parts of Speech**"""

from nltk import pos_tag , word_tokenize

texts = word_tokenize(text)
tags = token_tags = pos_tag(texts)
tags

"""**Named Entity Recognition**"""

from nltk import word_tokenize, pos_tag
from nltk.chunk import ne_chunk

tokens = word_tokenize(text)
tags = pos_tag(tokens)
print(ne_chunk(tags))

"""**Remove Stopwords**"""

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))
tokens = word_tokenize(text.lower())

filtered_tokens =[word for word in tokens if word not in stop_words]
print("original",tokens)
print("filtered",filtered_tokens)